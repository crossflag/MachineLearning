{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"pycharm":{"stem_cell":{"cell_type":"raw","source":[],"metadata":{"collapsed":false}}},"colab":{"name":"CS4347_Project1_dsp50.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"collapsed":false,"id":"K1dPEAso-AL3"},"source":["# Project: Transfer Learning\n","courtesy of `Sasank Chilamkurthy <https://chsasank.github.io>`\n","\n","In this project, we are trying to investigate transfer learning. \n","\n","You are going to do:\n","* experiments on finetuning the final fully connected layer\n","* experiments on freezing all the network except the final layer\n","* experiments on different architectures: resnet18, resnet50\n","\n","Just change some code in the last few cells can do all the experiments.\n","\n","Your report submission should contain:\n","* What scenarios do you think can transfer learning be use?\n","* Experiments on different optimizers and different learning_rates, and record the differences of running time and accuracy.\n","* please attach some screentshots.\n","\n","You can either write it in jupyter notebook or other file system. But do make sure you submit it.\n","\n","**Some useful links**:\n","* transfer learning: https://cs231n.github.io/transfer-learning\n","* pytorch: https://pytorch.org/\n","* pytorch pretrained models: https://pytorch.org/docs/stable/torchvision/models.html\n","* IPython Built-in magic commands: https://ipython.readthedocs.io/en/stable/interactive/magics.html\n","* Use free GPU in colab: https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm"]},{"cell_type":"code","metadata":{"id":"WO3myPxb-AL5"},"source":["%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"4Upn9DgZ-AL9"},"source":["# this mounts your Google Drive to the Colab VM.\n","from __future__ import print_function, division\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'Assignments/cs4347/'\n","FOLDERNAME = 'INTROtoMAC/Assignment1/cs4347'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","# this downloads the CIFAR-10 dataset to your Drive\n","# if it doesn't already exist.\n","%cd drive/My\\ Drive\n","%cp -r $FOLDERNAME ../../\n","%cd ../../\n","%cd cs4347/datasets/\n","!bash project_dataset.sh\n","%cd ../../\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"seWj310d-AL_"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","\n","plt.ion()   # interactive mode"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fom-GuWq-AMC"},"source":["Load Data\n","---------\n","\n","We will use torchvision and torch.utils.data packages for loading the\n","data.\n","\n","The problem we're going to solve today is to train a model to classify\n","**ants** and **bees**. We have about 120 training images each for ants and bees.\n","There are 75 validation images for each class. Usually, this is a very\n","small dataset to generalize upon, if trained from scratch. Since we\n","are using transfer learning, we should be able to generalize reasonably\n","well.\n","\n","This dataset is a very small subset of imagenet.  "]},{"cell_type":"code","metadata":{"id":"okYTCatz-AMD"},"source":["# Data augmentation and normalization for training\n","# Just normalization for validation\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","data_dir = '/content/cs4347/datasets/hymenoptera_data'\n","image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n","                                          data_transforms[x])\n","                  for x in ['train', 'val']}\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n","                                             shuffle=True, num_workers=4)\n","              for x in ['train', 'val']}\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n","class_names = image_datasets['train'].classes\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ednfchY0-AMF"},"source":["Visualize a few images\n","---------\n","Let's visualize a few training images so as to understand the data\n","augmentations.\n","\n"]},{"cell_type":"code","metadata":{"id":"n3wdfbD2-AMG"},"source":["def imshow(inp, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    inp = std * inp + mean\n","    inp = np.clip(inp, 0, 1)\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","\n","\n","# Get a batch of training data\n","inputs, classes = next(iter(dataloaders['train']))\n","\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs)\n","\n","imshow(out, title=[class_names[x] for x in classes])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mtMaYAe3-AMI"},"source":["Training the model\n","------------------\n","\n","Now, let's write a general function to train a model. Here, we will\n","illustrate:\n","\n","-  Scheduling the learning rate\n","-  Saving the best model\n","\n","In the following, parameter ``scheduler`` is an LR scheduler object from\n","``torch.optim.lr_scheduler``.\n","\n"]},{"cell_type":"code","metadata":{"id":"hPSnOtqx-AMJ"},"source":["def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","            if phase == 'train':\n","                scheduler.step()\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","                phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-eKm1Eim-AML"},"source":["Visualizing the model predictions\n","---------\n","\n","Generic function to display predictions for a few images\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"vcGrcxhN-AMM"},"source":["def visualize_model(model, num_images=6):\n","    was_training = model.training\n","    model.eval()\n","    images_so_far = 0\n","    fig = plt.figure()\n","\n","    with torch.no_grad():\n","        for i, (inputs, labels) in enumerate(dataloaders['val']):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","\n","            for j in range(inputs.size()[0]):\n","                images_so_far += 1\n","                ax = plt.subplot(num_images//2, 2, images_so_far)\n","                ax.axis('off')\n","                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n","                imshow(inputs.cpu().data[j])\n","\n","                if images_so_far == num_images:\n","                    model.train(mode=was_training)\n","                    return\n","        model.train(mode=was_training)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OM3JH8xg-AMS"},"source":["Finetuning the convnet\n","----------------------\n","\n","Load a pretrained model and reset final fully connected layer.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"KUm5RV6s-AMT"},"source":["model_ft = models.resnet50(pretrained=True)\n","num_ftrs = model_ft.fc.in_features\n","# Here the size of each output sample is set to 2.\n","# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n","model_ft.fc = nn.Linear(num_ftrs, 2)\n","\n","model_ft = model_ft.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","# Observe that all parameters are being optimized\n","optimizer_ft = optim.SGD(model_ft.parameters(), lr=.01, momentum=0.9)\n","\n","# Decay LR by a factor of 0.1 every 7 epochs\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ot0aeOja-AMV"},"source":["Train and evaluate\n","---------\n","\n","It should take around 15-25 min on CPU. On GPU though, it takes less than a\n","minute.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"pTQ8sfNw-AMW"},"source":["model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wNxOyB3g-AMY"},"source":["visualize_model(model_ft)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rxBRpL00-AMb"},"source":["ConvNet as fixed feature extractor\n","----------------------------------\n","\n","Here, we need to freeze all the network except the final layer. We need\n","to set ``requires_grad == False`` to freeze the parameters so that the\n","gradients are not computed in ``backward()``.\n","\n","You can read more about this in the documentation\n","`here <https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward>`__.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"g-xzvDf_-AMb"},"source":["model_conv = torchvision.models.resnet50(pretrained=True)\n","for param in model_conv.parameters():\n","    param.requires_grad = False\n","\n","# Parameters of newly constructed modules have requires_grad=True by default\n","num_ftrs = model_conv.fc.in_features\n","model_conv.fc = nn.Linear(num_ftrs, 2)\n","\n","model_conv = model_conv.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","# Observe that only parameters of final layer are being optimized as\n","# opposed to before.\n","optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=.1, momentum=0.8)\n","\n","# Decay LR by a factor of 0.1 every 7 epochs\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=10, gamma=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d2E97vxd-AMe"},"source":["Train and evaluate\n","---------\n","\n","On CPU this will take about half the time compared to previous scenario.\n","This is expected as gradients don't need to be computed for most of the\n","network. However, forward does need to be computed.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"oUDSG3bK-AMe"},"source":["model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"ag1LYUBV-AMg"},"source":["visualize_model(model_conv)\n","\n","plt.ioff()\n","plt.show()\n"],"execution_count":null,"outputs":[]}]}